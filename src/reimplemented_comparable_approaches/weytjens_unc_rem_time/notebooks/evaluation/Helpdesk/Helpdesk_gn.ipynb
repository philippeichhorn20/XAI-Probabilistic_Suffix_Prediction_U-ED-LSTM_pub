{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T18:13:16.528327Z",
     "iopub.status.busy": "2025-07-26T18:13:16.528244Z",
     "iopub.status.idle": "2025-07-26T18:13:17.223620Z",
     "shell.execute_reply": "2025-07-26T18:13:17.223314Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from typing import Optional\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "sys.path.insert(0, '../..')\n",
    "sys.path.insert(0, '../../..')\n",
    "sys.path.insert(0, '../../../..')\n",
    "sys.path.insert(0, '../../../../..')\n",
    "sys.path.insert(0, '../../../../../..')\n",
    "\n",
    "from stochasticLSTM.model import StochasticLSTMWeytjens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T18:13:17.224857Z",
     "iopub.status.busy": "2025-07-26T18:13:17.224730Z",
     "iopub.status.idle": "2025-07-26T18:13:17.750638Z",
     "shell.execute_reply": "2025-07-26T18:13:17.750319Z"
    }
   },
   "outputs": [],
   "source": [
    "#load model\n",
    "file_path_model = '../../training_variational_dropout/Helpdesk/Helpdesk_weytjens_rem_time_1_suffix_length5.pkl'\n",
    "\n",
    "model = StochasticLSTMWeytjens.load(file_path_model, p_fix=0.05, device='cpu')\n",
    "\n",
    "model_without_drop = StochasticLSTMWeytjens.load(file_path_model, p_fix=0, device='cpu')\n",
    "\n",
    "# Load the dataset\n",
    "file_path_data_set = '../../../../../../encoded_data/compare_weytjens/helpdesk_all_5_test.pkl'\n",
    "helpdesk_test_dataset = torch.load(file_path_data_set, weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T18:13:17.751981Z",
     "iopub.status.busy": "2025-07-26T18:13:17.751845Z",
     "iopub.status.idle": "2025-07-26T18:13:17.754208Z",
     "shell.execute_reply": "2025-07-26T18:13:17.754033Z"
    }
   },
   "outputs": [],
   "source": [
    "def force_model_cpu(model: torch.nn.Module) -> torch.nn.Module:\n",
    "    # move all parameters\n",
    "    for param in model.parameters():\n",
    "        param.data = param.data.cpu()\n",
    "        if param.grad is not None:\n",
    "            param.grad.data = param.grad.data.cpu()\n",
    "    # move all buffers (e.g. running_mean/running_var in BatchNorm)\n",
    "    for buf in model.buffers():\n",
    "        buf.data = buf.data.cpu()\n",
    "    return model\n",
    "\n",
    "model = force_model_cpu(model)\n",
    "model_without_drop = force_model_cpu(model_without_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T18:13:17.755059Z",
     "iopub.status.busy": "2025-07-26T18:13:17.754887Z",
     "iopub.status.idle": "2025-07-26T18:13:17.756839Z",
     "shell.execute_reply": "2025-07-26T18:13:17.756646Z"
    }
   },
   "outputs": [],
   "source": [
    "# Existing global placeholders (already provided)\n",
    "global_model = None\n",
    "\n",
    "global_model_without_drop = None\n",
    "\n",
    "global_samples_per_case = None\n",
    "\n",
    "global_act_categories = None\n",
    "\n",
    "global_scaler_params = None\n",
    "\n",
    "def init_worker(model, model_without_drop, samples_per_case, act_categories, scaler_params):\n",
    "    \"\"\"\n",
    "    Initializer for each worker process, setting global variables.\n",
    "    \"\"\"\n",
    "    global global_model, global_model_without_drop, global_samples_per_case, global_act_categories, global_scaler_params\n",
    "    \n",
    "    # Models have already been moved to CPU before forking (as per your comment)\n",
    "    model.eval()\n",
    "    model_without_drop.eval()\n",
    "    \n",
    "    global_model = model\n",
    "    global_model_without_drop = model_without_drop\n",
    "    global_samples_per_case = samples_per_case\n",
    "    global_act_categories = act_categories\n",
    "    global_scaler_params = scaler_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T18:13:17.757576Z",
     "iopub.status.busy": "2025-07-26T18:13:17.757498Z",
     "iopub.status.idle": "2025-07-26T18:13:17.760277Z",
     "shell.execute_reply": "2025-07-26T18:13:17.760080Z"
    }
   },
   "outputs": [],
   "source": [
    "def iterate_case(case: tuple[list[torch.Tensor], list[torch.Tensor]],\n",
    "                 concept_name_id: int,\n",
    "                 min_suffix_size: int):\n",
    "    \n",
    "    # Initialize prefix with zeros, matching the shape of the case tensors\n",
    "    current_prefix = (\n",
    "        [torch.zeros_like(cat_attribute).unsqueeze(0) for cat_attribute in case[0]],  # cats: one tensor for concept_name\n",
    "        [torch.zeros_like(num_attribute).unsqueeze(0) for num_attribute in case[1]]   # nums: one tensor for case_elapsed_time\n",
    "    )\n",
    "    \n",
    "    prefix_length = 0\n",
    "    seq_len = case[0][0].shape[0]  # Sequence length from the first tensor\n",
    "    \n",
    "    # Iterate up to seq_len - min_suffix_size - 1\n",
    "    for i in range(seq_len - min_suffix_size - 1):\n",
    "        # Update categorical attribute (concept_name)\n",
    "        for j in range(len(current_prefix[0])):  # j will be 0 since only one tensor\n",
    "            current_prefix[0][j][0] = torch.roll(current_prefix[0][j][0], -1)\n",
    "            current_prefix[0][j][0, -1] = case[0][j][i]\n",
    "        \n",
    "        # Update numerical attribute (case_elapsed_time)\n",
    "        for j in range(len(current_prefix[1])):  # j will be 0 since only one tensor\n",
    "            current_prefix[1][j][0] = torch.roll(current_prefix[1][j][0], -1)\n",
    "            current_prefix[1][j][0, -1] = case[1][j][i]\n",
    "        \n",
    "        # Yield prefix if it’s non-padding or prefix has started\n",
    "        if prefix_length or case[0][concept_name_id][i]:\n",
    "            prefix_length += 1\n",
    "            yield prefix_length, current_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T18:13:17.760983Z",
     "iopub.status.busy": "2025-07-26T18:13:17.760906Z",
     "iopub.status.idle": "2025-07-26T18:13:17.764321Z",
     "shell.execute_reply": "2025-07-26T18:13:17.764138Z"
    }
   },
   "outputs": [],
   "source": [
    "def _evaluate_case(case_name: str,\n",
    "                   full_case: tuple[list[torch.Tensor], list[torch.Tensor], str],\n",
    "                   concept_name_id: int,\n",
    "                   min_suffix_size: int):\n",
    "    \n",
    "    # Target\n",
    "    _, nums, _ = full_case    \n",
    "    # Target is the total elapsed time, same for all prefixes\n",
    "    mean_s, std_s = global_scaler_params\n",
    "    raw_target = nums[0][-1-min_suffix_size].item()\n",
    "    # print(raw_target)\n",
    "    target_val = raw_target * std_s + mean_s\n",
    "    target = [{'case_elapsed_time': target_val}]\n",
    "    \n",
    "    # Iterate over prefixes and targets from iterate_case\n",
    "    results = []\n",
    "    for prefix_length, prefix in iterate_case(full_case, concept_name_id, min_suffix_size):\n",
    "        \n",
    "        # Monte Carlo samples for uncertainty\n",
    "        mc_samples = []\n",
    "        for _ in range(global_samples_per_case):\n",
    "            # Get results of VI model:\n",
    "            mean, logvar = global_model(input=prefix)\n",
    "            mean = mean.squeeze(0)\n",
    "            std = torch.exp(0.5 * logvar).squeeze(0)\n",
    "            sample = torch.normal(mean=mean, std=std)\n",
    "            sample = sample * std_s + mean_s\n",
    "            sample = torch.clamp(sample, min=0.0)\n",
    "            mc_samples.append([{'case_elapsed_time': sample.item()}])\n",
    "        \n",
    "        # Deterministic prediction\n",
    "        # Get results from model with all activated neurons:\n",
    "        mean_cet, _ = global_model_without_drop(input=prefix)\n",
    "        mean_cet = mean_cet.squeeze(0)\n",
    "        mean_cet = torch.clamp(mean_cet * std_s + mean_s, min=0.0)\n",
    "        most_likely = [{'case_elapsed_time': mean_cet.item()}]\n",
    "        \n",
    "        # Prepare prefix in readable format (assuming first cat attribute is 'Activity')\n",
    "        prefix_cat = prefix[0][0]  # Shape: (1, seq_len)\n",
    "        act_categories = global_act_categories[0][2]\n",
    "        prefix_prep = []\n",
    "        for idx, cat in enumerate(prefix_cat[0].tolist()):\n",
    "            if cat != 0:\n",
    "                act = next(k for k, v in act_categories.items() if v == cat)\n",
    "                num_val = prefix[1][0][0, idx].item() \n",
    "                # print(num_val)\n",
    "                num_val = num_val * std_s + mean_s\n",
    "                prefix_prep.append({'Activity': act, 'case_elapsed_time': num_val})\n",
    "        \n",
    "        # print(\"\\n\")\n",
    "        # print(\"Case name:\", case_name)\n",
    "        # print(\"Prefix length: \", prefix_length)\n",
    "        # print(\"Prefix prepared: \", prefix_prep)\n",
    "        # print(\"MC samples: \", mc_samples)\n",
    "        # print(\"Target:\", target)\n",
    "        # print(\"Most likely: \", most_likely)\n",
    "        \n",
    "        results.append((case_name, prefix_length, prefix_prep, mc_samples, target, most_likely))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T18:13:17.765051Z",
     "iopub.status.busy": "2025-07-26T18:13:17.764970Z",
     "iopub.status.idle": "2025-07-26T18:13:17.767916Z",
     "shell.execute_reply": "2025-07-26T18:13:17.767738Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_seq_processing(model: StochasticLSTMWeytjens,\n",
    "                            model_without_drop: StochasticLSTMWeytjens,\n",
    "                            dataset,\n",
    "                            samples_per_case: Optional[int] = 1000,\n",
    "                            random_order: Optional[bool]= False):\n",
    "    \"\"\"\n",
    "    Sequential evaluation yielding tuples per case and prefix length.\n",
    "    \"\"\"\n",
    "    # Move models to CPU\n",
    "    model.to('cpu')\n",
    "    model_without_drop.to('cpu')\n",
    "    \n",
    "    # \n",
    "    concept_name = 'Activity'\n",
    "    concept_name_id = [i for i, cat in enumerate(dataset.all_categories[0]) if cat[0] == concept_name][0]\n",
    "    \n",
    "    # Id of EOS token in activity\n",
    "    eos_value = 'EOS'\n",
    "    eos_id = [v for k, v in dataset.all_categories[0][concept_name_id][2].items() if k == eos_value][0]\n",
    "    \n",
    "    cases = {}\n",
    "    for event in dataset:\n",
    "        # Get suffix being the last \n",
    "        suffix = event[0][concept_name_id][-dataset.encoder_decoder.min_suffix_size:]\n",
    "        if torch.all(suffix  == eos_id).item():\n",
    "            cases[event[2]] = event\n",
    "            \n",
    "    case_items = list(cases.items())\n",
    "    if random_order:\n",
    "        case_items = random.sample(case_items, len(case_items))\n",
    "    \n",
    "    cat_categories, _ = model.data_set_categories\n",
    "    scaler = dataset.encoder_decoder.continuous_encoders['case_elapsed_time']\n",
    "    scaler_params = (scaler.mean_.item(), scaler.scale_.item())\n",
    "    \n",
    "    # Initialize globals for identical logic\n",
    "    init_worker(model, model_without_drop, samples_per_case, cat_categories, scaler_params)\n",
    "    \n",
    "    # for cats, nums, case_name in tqdm(cases, total=len(cases)):\n",
    "    for _, (case_name, full_case) in tqdm(enumerate(case_items), total=len(cases)):\n",
    "        \n",
    "        # Get a list with the results for all cases of one case:\n",
    "        results = _evaluate_case(case_name, full_case, min_suffix_size=dataset.encoder_decoder.min_suffix_size, concept_name_id=concept_name_id)\n",
    "        \n",
    "        for res in results:\n",
    "            yield res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T18:13:17.768687Z",
     "iopub.status.busy": "2025-07-26T18:13:17.768545Z",
     "iopub.status.idle": "2025-07-26T18:13:17.771882Z",
     "shell.execute_reply": "2025-07-26T18:13:17.771702Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_parallel_processing(model: StochasticLSTMWeytjens,\n",
    "                                 model_without_drop: StochasticLSTMWeytjens,\n",
    "                                 dataset,\n",
    "                                 samples_per_case: Optional[int] = 1000,\n",
    "                                 random_order: Optional[bool] = False,\n",
    "                                 num_processes: Optional[int] = 4):\n",
    "\n",
    "    # 1) Move models to CPU\n",
    "    model.to('cpu')\n",
    "    model_without_drop.to('cpu')\n",
    "\n",
    "    # 2) Find your IDs (same logic as before)\n",
    "    concept_name = 'Activity'\n",
    "    concept_name_id = next(i for i, cat in enumerate(helpdesk_test_dataset.all_categories[0]) if cat[0] == concept_name)\n",
    "    \n",
    "    eos_value = 'EOS'\n",
    "    eos_id = next(v for k, v in helpdesk_test_dataset.all_categories[0][concept_name_id][2].items()if k == eos_value)\n",
    "\n",
    "    # 3) Collect only “finished” cases\n",
    "    cases = {}\n",
    "    for event in dataset:\n",
    "        suffix = event[0][concept_name_id][-dataset.encoder_decoder.min_suffix_size:]\n",
    "        if torch.all(suffix == eos_id).item():\n",
    "            cases[event[2]] = event\n",
    "\n",
    "    case_items = list(cases.items())\n",
    "    if random_order:\n",
    "        random.shuffle(case_items)\n",
    "\n",
    "    # 4) Extract constants for the workers\n",
    "    min_suffix_size = dataset.encoder_decoder.min_suffix_size\n",
    "    cat_categories, _ = model.data_set_categories\n",
    "    \n",
    "    scaler = dataset.encoder_decoder.continuous_encoders['case_elapsed_time']\n",
    "    scaler_params = (scaler.mean_.item(), scaler.scale_.item())\n",
    "\n",
    "    # Global variables each worker can use, that are not changed over time:\n",
    "    init_args = (model, model_without_drop, samples_per_case, cat_categories, scaler_params)\n",
    "\n",
    "    # Inputs for _evaluate_case method\n",
    "    pool_inputs = [(case_name, full_case, concept_name_id, min_suffix_size) for case_name, full_case in case_items]\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=num_processes,\n",
    "                             initializer=init_worker,\n",
    "                             initargs=init_args) as executor:\n",
    "        \n",
    "        # Submit all cases at once\n",
    "        futures = {executor.submit(_evaluate_case, *args): args for args in pool_inputs}\n",
    "\n",
    "        # As each one completes, yield its individual results\n",
    "        for fut in tqdm(as_completed(futures), total=len(futures), desc=\"Parallel eval\"):\n",
    "            case_name = futures[fut]\n",
    "            case_results = fut.result()  # this is the list returned by _evaluate_case\n",
    "            for res in case_results:\n",
    "                yield res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T18:13:17.772563Z",
     "iopub.status.busy": "2025-07-26T18:13:17.772481Z",
     "iopub.status.idle": "2025-07-26T18:13:17.774175Z",
     "shell.execute_reply": "2025-07-26T18:13:17.774006Z"
    }
   },
   "outputs": [],
   "source": [
    "output_dir = '../../../../../../../../data/Helpdesk/eval_weytjens_sl5/'\n",
    "\n",
    "def save_chunk(results, i):\n",
    "    chunk_number = (i + 1)\n",
    "    filename = os.path.join(output_dir, f'results_part_{chunk_number:03d}.pkl')\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    print(f\"Saved {len(results)} results to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T18:13:17.774870Z",
     "iopub.status.busy": "2025-07-26T18:13:17.774793Z",
     "iopub.status.idle": "2025-07-26T20:22:01.385016Z",
     "shell.execute_reply": "2025-07-26T20:22:01.384658Z"
    }
   },
   "outputs": [],
   "source": [
    "num_processes=16\n",
    "save_every = 50\n",
    "results = {}\n",
    "\n",
    "# for i, (case_name, prefix_len, prefix, sampled_cets, target_cet, mean_cet) in enumerate(evaluate_seq_processing(model=model,\n",
    "#                                                                                                                 model_without_drop=model_without_drop,\n",
    "#                                                                                                                 dataset=helpdesk_test_dataset)):\n",
    "\n",
    "for i, (case_name, prefix_len, prefix, sampled_cets, target_cet, mean_cet) in enumerate(evaluate_parallel_processing(model=model,\n",
    "                                                                                                                     model_without_drop=model_without_drop,\n",
    "                                                                                                                     dataset=helpdesk_test_dataset)):\n",
    "    \n",
    "    # print(case_name, prefix_len)\n",
    "    # if (case_name != '1016'):\n",
    "    #    break\n",
    "    \n",
    "    assert((case_name, prefix_len) not in results)\n",
    "    \n",
    "    results[(case_name, prefix_len)] = (prefix, target_cet, mean_cet, sampled_cets)\n",
    "    \n",
    "    if (i + 1) % save_every == 0:\n",
    "        # save_chunk(results, i)\n",
    "        print(i)\n",
    "        results = {}\n",
    "\n",
    "if len(results):\n",
    "    pass\n",
    "    #save_chunk(results, i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PrimePPM-tDGhFIeG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0f1e5fa1289a4973aee01a132fadb2e5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "18fbee425ac74dffbe02d1fde657909a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2c4d1639a2f54154b0de587dcb0272ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_39f7f482327a467bb8783c1e45ed2e63",
       "placeholder": "​",
       "style": "IPY_MODEL_3d2f3c820a964a63b10ebc5839383c13",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "39f7f482327a467bb8783c1e45ed2e63": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3d2f3c820a964a63b10ebc5839383c13": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "48b687d9bde44d8b94f60131ff03a7e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8972ab140e62469383d81f5f9a0b322c",
       "placeholder": "​",
       "style": "IPY_MODEL_18fbee425ac74dffbe02d1fde657909a",
       "tabbable": null,
       "tooltip": null,
       "value": " 535/535 [52:16&lt;00:00,  6.61s/it]"
      }
     },
     "65c54b4ee754446a87489889e33bed7f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "76473ba04feb461d929b58538bb8f313": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_65c54b4ee754446a87489889e33bed7f",
       "max": 535,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ffc6385fe6594114ace93c0ba03781bd",
       "tabbable": null,
       "tooltip": null,
       "value": 535
      }
     },
     "8972ab140e62469383d81f5f9a0b322c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9be34aa604f54b8eb8d02b837bf975a2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_2c4d1639a2f54154b0de587dcb0272ef",
        "IPY_MODEL_76473ba04feb461d929b58538bb8f313",
        "IPY_MODEL_48b687d9bde44d8b94f60131ff03a7e4"
       ],
       "layout": "IPY_MODEL_0f1e5fa1289a4973aee01a132fadb2e5",
       "tabbable": null,
       "tooltip": null
      }
     },
     "ffc6385fe6594114ace93c0ba03781bd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
